# Aula dia 17 de Novembro

*Resumindo aula passada:*

$\begin{cases}
    x' = F(t,x) \\
    x(t_0) = x_0
\end{cases}$

Intervalo: $[t_0, T]$

Todo método deve satisfazer:

$x'(t) = \lambda x(t), \lambda \in \mathbb{C}^{-}$

$\rightarrow x_{n+1} = R(h\lambda)x_n$

A-estável $\rightarrow$ $S = \{z \in \mathbb{C} : |R(z)| \leq 1\} \supset \mathbb{C}^{-}$

---

*O que temos que saber:* Dado um método, quero saber se o método é A-estável. Caso não seja, que $h$ escolher para que seja A-estável.

Método: $|R(z)| \leq 1$

Sistema: $x' = Ax$

Autovaolres de $A$: $\lambda_1, \dots, \lambda_n$

Escolher $h$ tal que $|R(\lambda_i h)| \leq 1$

Mas, nem sempre é possível escolher $h$ para que $|R(\lambda_i h)| \leq 1$.

**Exemplo simples:** Oscilador harmônico.

$x'(t) = \begin{pmatrix}
    0 & 1 \\
    -w^2 & 0
\end{pmatrix} x(t)$

$\Rightarrow \begin{bmatrix}
    w x_1(t) \\
    x_2(t)
\end{bmatrix}' = \begin{bmatrix}
    0 & 1 \\
    -w^2 & 0
\end{bmatrix} \begin{bmatrix}
    w x_1(t) \\
    x_2(t)
\end{bmatrix}$

$\begin{bmatrix}
    x_1(t) \\
    x_2(t)
\end{bmatrix} = e^{\begin{bmatrix}
    0 & w \\
    -w & 0
\end{bmatrix} t} \begin{bmatrix}
    w x_1(0) \\
    x_2(0)
\end{bmatrix}$

$\begin{bmatrix}
    w x_1(t) \\
    x_2(t)
\end{bmatrix} = \begin{bmatrix}
    \cos(wt) & \sin(wt) \\
    -\sin(wt) & \cos(wt)
\end{bmatrix} \begin{bmatrix}
    w x_1(0) \\
    x_2(0)
\end{bmatrix}$

$\begin{bmatrix}
w x_1(t) & x_2(t) \end{bmatrix} = \begin{bmatrix}
    w x_1(t) \\
    x_2(t)
\end{bmatrix} = 
\begin{bmatrix}
    w x_1(0) & x_2(0)
\end{bmatrix} \begin{bmatrix}
    \cos(wt) & -\sin(wt) \\
    \sin(wt) & \cos(wt)
\end{bmatrix} \begin{bmatrix}
  \cos(wt) & \sin(wt) \\
    -\sin(wt) & \cos(wt)
\end{bmatrix} \begin{bmatrix}
    w x_1(0) \\
    x_2(0)
\end{bmatrix}$

$\Rightarrow || \begin{bmatrix}
    w x_1(t) \\
    x_2(t) \end{bmatrix} || = || \begin{bmatrix}
    w x_1(0) \\
    x_2(0)
\end{bmatrix} ||, \forall t \in \mathbb{R}$

$w^2 x_1^2(t) + x_2^2(t) = w^2 x_1^2(0) + x_2^2(0)$

$\frac{x_1^2(t)}{\frac{w^2 x_1^2(0) + x_2^2(0)}{w^2}} + \frac{x_2^2(t)}{w^2 x_1^2(0) + x_2^2(0)} = 1$

Código que não serve para muita coisa:

```MATLAB
w = 1;
t = 0:0.01:10;
x1 = cos(w*t);
x2 = sin(w*t);
plot(x1, x2)
```

$x'(t) = ibx(t)$

$x_{n+1} = (1 + ihb)x_n$

$|1 + ihb| = \sqrt{1 + h^2b^2} \geq 1$ Não existe $h$ tal que $|1 + ihb| \leq 1$

Lista e prova:

O que tem que saber:

- Considere tal método, prove que converge com ordem p. (aplicar teorema, exemplo como foi usado para Trapézio)
- Para qual valor de h esse método funciona bem. (na lista tem exatamente isso)
- Esse método reproduz a dinâmica para todo h? A-estável ou não.
- Não tem questão de habilidade de matemática, é mais de entender o que está acontecendo.
- Na lista os problemas são mais difíceis que os da prova.

---

## Solução númerica de EDP

Existem vários tipos de métodos, o que veremos nesse curso:
- Método de diferenças finitas


Tudo que iremos fazer é para equações assim:

$Au_{xx} + Bu_{xt} + Cu_{tt} = F(x,t, u, u_x, u_t)$

**Objetivo:** encontrar $u(x,t)$

**Focando na equação do calor:**

$u_t = c u_{xx}$

*Condição inicial:* 

$u(x,0) = f(x)$, $0 \leq x \leq L$

*Condição de fronteira:* 

$u(0,t) = a(t) \\
u(L,t) = b(t)$, $0 \leq t \leq T$

**Ideia:** Particionar o espaço e o tempo (Grid, ou uma malha de um plano com eixos x e t, espaçados de tamanho $\Delta x$ e $\Delta t$)

$U^i_j \approx u(j \Delta x, i \Delta t)$


Queremos que: $\lim_{\Delta x, \Delta t \rightarrow 0} | u(x_j, t_i) - U^i_j | = 0$

Como construir aproximações nesses pontos? (Ver foto)

*Próxima aula:* 

Quero aproximar $g'(t_i)$ a partir dos pontos vizinhos e calcular o erro.

Por Taylor: 

$g'(t_i) = \frac{g(t_i + \Delta t) - g(t_i)}{\Delta t} + O(\Delta t)$ (Forward)

$g'(t_i) = \frac{g(t_i) - g(t_i - \Delta t)}{\Delta t} + O(\Delta t)$ (Backward)

$g(x_{i+1}) = g(x_i) + g'(x_i) \Delta x + \frac{g''(x_i)}{2} \Delta x^2 + \frac{g'''(x_i)}{6} \Delta x^3 + O(\Delta x^4)$

$g(x_{i-1}) = g(x_i) - g'(x_i) \Delta x + \frac{g''(x_i)}{2} \Delta x^2 - \frac{g'''(x_i)}{6} \Delta x^3 + O(\Delta x^4)$

A ideia é que quando somamos isso, ficamos com:

$g(x_{i+1}) + g(x_{i-1}) = 2g(x_i) + g''(x_i) \Delta x^2 + O(\Delta x^4)$

$\Rightarrow g''(x_i) = \frac{g(x_{i+1}) - 2g(x_i) + g(x_{i-1})}{\Delta x^2} + O(\Delta x^2)$ (Aproximação da segunda derivada com ordem 2)




